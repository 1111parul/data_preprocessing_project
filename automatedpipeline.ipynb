{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1SifSRhZ8jzxINZoyQz4P0DfDqfB3L0jg",
      "authorship_tag": "ABX9TyPrV2FmwewGuUkIvQVfWz4m"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "class DateParser:\n",
        "    \"\"\"Parse date columns and create temporal features\"\"\"\n",
        "    def __init__(self):\n",
        "        self.date_columns = []\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.date_columns = []\n",
        "        for col in X.columns:\n",
        "            try:\n",
        "                pd.to_datetime(X[col], errors='raise')\n",
        "                self.date_columns.append(col)\n",
        "            except:\n",
        "                pass\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transforms the input data by parsing date columns and creating temporal features.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): The input data.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: The transformed data with date columns parsed and temporal features added.\n",
        "        \"\"\"\n",
        "        X = X.copy()\n",
        "        for col in self.date_columns:\n",
        "            X[col] = pd.to_datetime(X[col], errors='coerce')\n",
        "            X[f'{col}_year'] = X[col].dt.year\n",
        "            X[f'{col}_month'] = X[col].dt.month\n",
        "            X[f'{col}_day'] = X[col].dt.day\n",
        "        return X.drop(columns=self.date_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QRdvQKv2Sk1",
        "outputId": "e7cbe81c-1e33-4fb5-a150-f5e8a1004d96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextCleaner:\n",
        "    \"\"\"Clean and preprocess text columns\"\"\"\n",
        "    def __init__(self, lang='english'):\n",
        "        self.stop_words = set(stopwords.words(lang))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        tokens = text.split()\n",
        "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens\n",
        "                 if word not in self.stop_words]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def transform(self, X):\n",
        "        for col in X.select_dtypes(include=['object']).columns:\n",
        "            X[col] = X[col].apply(self.clean_text)\n",
        "        return X"
      ],
      "metadata": {
        "id": "X5zsuUT67U-F"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample dataset\n",
        "data = {\n",
        "    'age': [25, 30, 35, None, 40, 150, 45],\n",
        "    'salary': [50000, 60000, None, 70000, 80000, 90000, 1000000],\n",
        "    'gender': ['M', 'F', 'F', 'M', None, 'F', 'M'],\n",
        "    'city': ['New York', 'Los Angeles', 'NY', 'LA', 'NYC', 'San Fran', ''],\n",
        "    'purchase_date': ['2023-01-15', '2022-13-01', '2021-07-23',\n",
        "                    'invalid', '2020-05-12', None, '2023-03-30']\n",
        "}\n",
        "df = pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "Lq10cU8F2rBl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Date parsing\n",
        "date_parser = DateParser()\n",
        "date_parser.fit(df)  # Fit the DateParser\n",
        "df = date_parser.transform(df)  # Transform the data\n",
        "\n",
        "# 2. Text cleaning\n",
        "text_cleaner = TextCleaner()\n",
        "df = text_cleaner.transform(df)\n",
        "\n",
        "# 3. Handle missing values\n",
        "num_cols = df.select_dtypes(include='number').columns\n",
        "cat_cols = df.select_dtypes(include='object').columns\n",
        "\n",
        "# Numerical imputation\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
        "\n",
        "# Categorical imputation\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent', fill_value='missing')\n",
        "df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
        "\n",
        "# 4. Outlier handling using IQR\n",
        "for col in num_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "\n",
        "# Display cleaned data\n",
        "print(\"Cleaned Data:\")\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKJKzbI926xu",
        "outputId": "71d4aed6-edff-4a37-9feb-e02aaaf22d91"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-e7882a2cc7a2>:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  pd.to_datetime(X[col], errors='raise')\n",
            "<ipython-input-1-e7882a2cc7a2>:21: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  pd.to_datetime(X[col], errors='raise')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Data:\n",
            "  gender         city purchase_date  age_year  age_month  age_day  \\\n",
            "0            new york                  1970.0        1.0      1.0   \n",
            "1      f  los angeles                  1970.0        1.0      1.0   \n",
            "2      f           ny                  1970.0        1.0      1.0   \n",
            "3                  la       invalid    1970.0        1.0      1.0   \n",
            "4   none          nyc                  1970.0        1.0      1.0   \n",
            "5      f     san fran          none    1970.0        1.0      1.0   \n",
            "6                                      1970.0        1.0      1.0   \n",
            "\n",
            "   salary_year  salary_month  salary_day  \n",
            "0       1970.0           1.0         1.0  \n",
            "1       1970.0           1.0         1.0  \n",
            "2       1970.0           1.0         1.0  \n",
            "3       1970.0           1.0         1.0  \n",
            "4       1970.0           1.0         1.0  \n",
            "5       1970.0           1.0         1.0  \n",
            "6       1970.0           1.0         1.0  \n"
          ]
        }
      ]
    }
  ]
}